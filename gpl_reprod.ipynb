{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bc48e6-2d22-43f6-b2cc-f5b3d62f882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.4.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (66.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.30.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (15.0.6.1)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.51.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (23.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2019.11.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
      "Installing collected packages: flatbuffers, tensorflow-hub, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow_text\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.2\n",
      "    Uninstalling tensorflow-2.9.2:\n",
      "      Successfully uninstalled tensorflow-2.9.2\n",
      "Successfully installed flatbuffers-23.1.21 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflow_text-2.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.1.21)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gpl\n",
      "  Downloading gpl-0.1.4-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from gpl) (3.19.6)\n",
      "Collecting easy-elasticsearch>=0.0.9\n",
      "  Downloading easy_elasticsearch-0.0.9-py3-none-any.whl (12 kB)\n",
      "Collecting beir\n",
      "  Downloading beir-1.0.1.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.9/dist-packages (from gpl) (7.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from easy-elasticsearch>=0.0.9->gpl) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from easy-elasticsearch>=0.0.9->gpl) (4.64.1)\n",
      "Collecting elasticsearch>=7.9.1\n",
      "  Downloading elasticsearch-8.6.2-py3-none-any.whl (385 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.4/385.4 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (from beir->gpl) (2.2.2)\n",
      "Collecting pytrec_eval\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss_cpu\n",
      "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting elasticsearch>=7.9.1\n",
      "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from beir->gpl) (2.4.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from elasticsearch>=7.9.1->easy-elasticsearch>=0.0.9->gpl) (1.26.14)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from elasticsearch>=7.9.1->easy-elasticsearch>=0.0.9->gpl) (2019.11.28)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest->gpl) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest->gpl) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest->gpl) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest->gpl) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytest->gpl) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (1.23.4)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (0.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (3.8.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (1.5.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (0.3.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->beir->gpl) (2023.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->easy-elasticsearch>=0.0.9->gpl) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->easy-elasticsearch>=0.0.9->gpl) (2.8)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (3.7)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (0.1.97)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (1.1.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (0.13.1+cu116)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (4.21.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (1.9.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->beir->gpl) (1.12.1+cu116)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->beir->gpl) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->beir->gpl) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->beir->gpl) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->beir->gpl) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->beir->gpl) (1.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir->gpl) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir->gpl) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir->gpl) (4.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->beir->gpl) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->beir->gpl) (0.12.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers->beir->gpl) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers->beir->gpl) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->beir->gpl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->beir->gpl) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers->beir->gpl) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers->beir->gpl) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->beir->gpl) (1.14.0)\n",
      "Building wheels for collected packages: beir, pytrec_eval\n",
      "  Building wheel for beir (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for beir: filename=beir-1.0.1-py3-none-any.whl size=62501 sha256=7e16fbb8c573649fe1c40b066cd965a46cf6ff6ebd438c4519a4b7afcfd8731b\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/8a/1c/5a3160fba6f8fdd7c0ddc011751161544d1d98515b0c573d53\n",
      "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp39-cp39-linux_x86_64.whl size=293178 sha256=17b9fc87628798fe68ac6679af9d8118433d64b7c11faca5fe107a2cc1c27fce\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/dc/93/02e4422dd53e36b0c5005832495bb24e97eee09fd626e866ff\n",
      "Successfully built beir pytrec_eval\n",
      "Installing collected packages: faiss_cpu, pytrec_eval, elasticsearch, attrs, easy-elasticsearch, beir, gpl\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 22.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-22.2.0 beir-1.0.1 easy-elasticsearch-0.0.9 elasticsearch-7.9.1 faiss_cpu-1.7.3 gpl-0.1.4 pytrec_eval-0.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.21.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.12.1+cu116)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.9.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.13.1+cu116)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text\n",
    "!pip install tensorflow\n",
    "!pip install gpl\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131dc3f7-af8a-492c-aa70-cd74b70f9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 13:48:26.087016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 13:48:27.015374: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64\n",
      "2023-02-24 13:48:27.015503: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64\n",
      "2023-02-24 13:48:27.015511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-24 13:48:29 - Loading faiss with AVX2 support.\n",
      "2023-02-24 13:48:29 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import json\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import gpl\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39d5114-9b84-49e5-94e3-b1dd1c091521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to perform pre-processing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9,.'?]+\", ' ', str(text))\n",
    "    text = [text[:len(text)//2],text[len(text)//2:]]\n",
    "    return text\n",
    "\n",
    "def split_first(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def convert_str(text):\n",
    "    return str(text)+\"_\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd42d16-5514-4f8a-b7a3-55bc00294d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x:x.str.lower())\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['new_text']= df.Resume.apply(preprocess)\n",
    "\n",
    "df = df.explode(\"new_text\")\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df[\"_id\"] = df.index\n",
    "\n",
    "df['num']=df['new_text'].apply(split_first)\n",
    "\n",
    "df = df[df['num']<400]\n",
    "\n",
    "df[\"_id\"] = df[\"_id\"].apply(convert_str)\n",
    "\n",
    "df['title'] = \"\"\n",
    "\n",
    "df['metadata'] = \"\"\n",
    "\n",
    "df['title'] = df['title'].astype(str)\n",
    "\n",
    "df['text'] = df['new_text'].astype(str)\n",
    "\n",
    "df['_id'] = df['_id'].astype(str)\n",
    "\n",
    "df['concat'] = \"qgen\" + df[\"title\"] + \" \" + df[\"text\"]\n",
    "\n",
    "df[['_id', 'title', 'text', 'metadata']].to_json('corpus.json',orient='records')\n",
    "\n",
    "df[['_id', 'title', 'text', 'metadata']].to_json('corpus.jsonl',orient='records',lines=True)\n",
    "\n",
    "\n",
    "\n",
    "f = open('corpus.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe1984-079e-44bd-9e73-d71da312aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('corpus.jsonl', 'w') as outfile:\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b34d11-aa80-4a5b-a0e7-dbe6ec1777c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the correct format\n",
    "filepath = 'corpus.jsonl'\n",
    "\n",
    "\n",
    "with open(filepath, 'r') as infile, open('output.json', 'w') as outfile:\n",
    "    data = [json.loads(line) for line in infile]\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a426258-a60e-4178-8c4e-fc6c3e8ec411",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to csv\n",
    "df = pd.read_json('output.json')\n",
    "\n",
    "df['num']=df['text'].apply(split_first)\n",
    "\n",
    "df = df[df['num']<400]\n",
    "\n",
    "df.to_csv('final_.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c44407-ddb4-4ba7-9321-f8d5ab88e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv('final_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5ca33-84f6-45d0-80dc-0ea56d9f27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Generation via T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'doc2query/msmarco-t5-base-v1'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "## load the correct model class like t5forseq2seq or conditional?\n",
    "print(tokenizer.__class__.__name__)\n",
    "print(model.__class__.__name__)\n",
    "\n",
    "passage = df['text'].iloc[5]\n",
    "\n",
    "# tokenize the passage\n",
    "inputs = tokenizer(passage, return_tensors='pt')\n",
    "# generate three queries\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=64,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Paragraph:\")\n",
    "print(passage)\n",
    "\n",
    "print(\"\\nGenerated Queries:\")\n",
    "for i in range(len(outputs)):\n",
    "    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "    print(f'{i + 1}: {query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d787d92-057e-4488-8726-2f57fa4186d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "target = 200_000\n",
    "batch_size = 128 #256\n",
    "num_queries = 3  # number of queries to generate for each passage\n",
    "count = 0\n",
    "lines = []\n",
    "passage_batch = []\n",
    "\n",
    "# reinitialize passage generator\n",
    "passages = df['text'].to_numpy() #get_text()\n",
    "\n",
    "with tqdm(total=target) as progress:\n",
    "    for passage in passages:\n",
    "        if count >= target: break\n",
    "        # remove tab + newline characters if present\n",
    "        passage_batch.append(passage.replace('\\t', ' ').replace('\\n', ' '))\n",
    "        \n",
    "        # we encode in batches\n",
    "        if len(passage_batch) == batch_size:\n",
    "            # tokenize the passage\n",
    "            inputs = tokenizer(\n",
    "                passage_batch,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # generate three queries per doc/passage\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'].cuda(),\n",
    "                attention_mask=inputs['attention_mask'].cuda(),\n",
    "                max_length=64,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=num_queries\n",
    "            )\n",
    "\n",
    "            # decode query to human readable text\n",
    "            decoded_output = tokenizer.batch_decode(\n",
    "                outputs,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # loop through to pair query and passages\n",
    "            for i, query in enumerate(decoded_output):\n",
    "                query = query.replace('\\t', ' ').replace('\\n', ' ')  # remove newline + tabs\n",
    "                passage_idx = int(i/num_queries)  # get index of passage to match query\n",
    "                lines.append(query+'\\t'+passage_batch[passage_idx])\n",
    "                count += 1\n",
    "            \n",
    "            passage_batch = []\n",
    "            progress.update(len(decoded_output))\n",
    "\n",
    "# write (Q, P+) pairs to file\n",
    "with open('pairs.tsv', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7e14d-2c9c-4744-81e2-c3b9d98d2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pairs.tsv', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be49ad-1a5e-43da-9139-06759ad1ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative sampling\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('msmarco-distilbert-base-tas-b')\n",
    "model.max_seq_length = 256\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_text():\n",
    "    with open('pairs.tsv', 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.read().split('\\n')\n",
    "    for line in tqdm(lines):\n",
    "        try:\n",
    "            query, passage = line.split('\\t')\n",
    "            yield query, passage\n",
    "        except ValueError:\n",
    "            # in case of malformed data, pass onto next row\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce70a32-3a2b-45ce-b157-ba684362df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pair_gen = get_text()\n",
    "\n",
    "for i, (query, passage) in enumerate(pair_gen):\n",
    "    print(query)\n",
    "    print()\n",
    "    print(passage)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244b343-96af-40bf-9a0a-87fd935ecd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings \n",
    "\n",
    "! pip install pinecone-client\n",
    "\n",
    "import pinecone  # pip install pinecone-client\n",
    "\n",
    "API_KEY = \"\"  # get api key app.pinecone.io\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=API_KEY,\n",
    "    environment=\"us-east1-gcp\"  # find next to API key in console\n",
    ")\n",
    "# create a new negative mining index if does not already exist\n",
    "if 'negative-mine' not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        'negative-mine',\n",
    "        dimension=model.get_sentence_embedding_dimension(),\n",
    "        metric='dotproduct',\n",
    "        pods=1\n",
    "    )\n",
    "# connect\n",
    "index = pinecone.Index('negative-mine')\n",
    "\n",
    "pinecone.describe_index(\"negative-mine\")\n",
    "\n",
    "\n",
    "\n",
    "pair_gen = get_text()\n",
    "\n",
    "pairs = []\n",
    "to_upsert = []\n",
    "passage_batch = []\n",
    "id_batch = []\n",
    "batch_size = 64\n",
    "\n",
    "for i, (query, passage) in enumerate(pair_gen):\n",
    "    pairs.append((query, passage))\n",
    "    # we do this to avoid passage duplication in the vector DB\n",
    "    if passage not in passage_batch: \n",
    "        passage_batch.append(passage)\n",
    "        id_batch.append(str(i))\n",
    "    # on reaching batch_size, we encode and upsert\n",
    "    if len(passage_batch) == batch_size:\n",
    "        embeds = model.encode(passage_batch).tolist()\n",
    "        # upload to index\n",
    "        index.upsert(vectors=list(zip(id_batch, embeds)))\n",
    "        # refresh batches\n",
    "        passage_batch = []\n",
    "        id_batch = []\n",
    "        \n",
    "# check number of vectors in the index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b471c4-95e4-47aa-9cfc-f2d33cdf57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataset\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "batch_size = 100\n",
    "triplets = []\n",
    "\n",
    "for i in tqdm(range(0, len(pairs), batch_size)):\n",
    "    # embed queries and query pinecone in batches to minimize network latency\n",
    "    i_end = min(i+batch_size, len(pairs))\n",
    "    queries = [pair[0] for pair in pairs[i:i_end]] #100\n",
    "    pos_passages = [pair[1] for pair in pairs[i:i_end]] #100\n",
    "    # create query embeddings\n",
    "    query_embs = model.encode(queries, convert_to_tensor=True, show_progress_bar=False) #100*764\n",
    "    # search for top_k most similar passages\n",
    "\n",
    "    res = dict()\n",
    "    res['results'] = [index.query(i.tolist(), top_k=10) for i in query_embs]\n",
    "\n",
    "    \n",
    "    # iterate through queries and find negatives\n",
    "    for query, pos_passage, query_res in zip(queries, pos_passages, res['results']):\n",
    "        top_results = query_res['matches']\n",
    "        # shuffle results so they are in random order\n",
    "        random.shuffle(top_results)\n",
    "        for hit in top_results:\n",
    "            neg_passage = pairs[int(hit['id'])][1]\n",
    "            # check that we're not just returning the positive passage\n",
    "            if neg_passage != pos_passage:\n",
    "                # if not we can add this to our (Q, P+, P-) triplets\n",
    "                triplets.append(query+'\\t'+pos_passage+'\\t'+neg_passage)\n",
    "                break\n",
    "\n",
    "with open('triplets.tsv', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30af68-145c-4fe6-a073-f92fe08300b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 triplets.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfdaf5-b4db-487f-bc00-5a2da229bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the negative and positive examples\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bed58c-6c1d-469a-882a-476f36ea0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_lines():\n",
    "    # loop through each file\n",
    "    with open('triplets.tsv', 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.read().split('\\n')\n",
    "    # loop through each line in the current file\n",
    "    for line in tqdm(lines):\n",
    "        q, p, n = line.split('\\t')\n",
    "        # return the query, positive, negative\n",
    "        yield q, p, n\n",
    "        \n",
    "lines = get_lines()\n",
    "label_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    q, p, n = line\n",
    "    p_score = model.predict((q, p))\n",
    "    n_score = model.predict((q, n))\n",
    "    margin = p_score - n_score\n",
    "    # append pairs to label_lines with margin score\n",
    "    label_lines.append(\n",
    "        q + '\\t' + p + '\\t' + n + '\\t' + str(margin)\n",
    "    )\n",
    "\n",
    "with open(\"triplets_margin.tsv\", 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(label_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad939e75-41c9-4c11-9c2f-ff003e68ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "training_data = []\n",
    "\n",
    "with open('triplets_margin.tsv', 'r', encoding='utf-8') as fp:\n",
    "    lines = fp.read().split('\\n')\n",
    "# loop through each line and return InputExample\n",
    "for line in tqdm(lines):\n",
    "    q, p, n, margin = line.split('\\t')\n",
    "    training_data.append(InputExample(\n",
    "        texts=[q, p, n],\n",
    "        label=float(margin)\n",
    "    ))\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8233c6e-394d-4299-a31c-2b7407f283be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    training_data, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76638a4-d840-4063-9a1e-fa2a1a539456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('msmarco-distilbert-base-tas-b')\n",
    "model.max_seq_length = 256\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac5a1f-41c6-4d71-b534-20979554c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "loss = losses.MarginMSELoss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109c403-bdb7-49e4-b786-c801f04fe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='msmarco-distilbert-base-tas-b-final',\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e508f0-59d3-4081-8415-eeca186b2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-02-24 14:17:43] INFO [sentence_transformers.SentenceTransformer.__init__:66] Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b-final\n",
      "[2023-02-24 14:17:44] INFO [sentence_transformers.SentenceTransformer.__init__:105] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('msmarco-distilbert-base-tas-b-final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a3581-4d86-45cf-a969-3f385d35c68f",
   "metadata": {},
   "source": [
    "### query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ef3c6-677a-4dd0-adb2-6692da5af50a",
   "metadata": {},
   "source": [
    "### document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51d0afbb-ee63-4a70-964f-25937621459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have 3.5+ years of work experience and was working as a data scientist with 3 different organizations. I was responsible for using predictive modelling, data processing, and data mining algorithms to solve challenging business problems.\\nMy technology stack includes but not limited to, are python, machine learning, deep learning, time-series, web scraping, flask, FastAPI, snowflake SQL servers, deploying production based servers, keras, TensorFlow, hugging face, Big Data and Data Warehouses. In my career, my growth has been exponential, and I developed interpersonal skills, now I know how to handle a project end to end.\\nMy area of interests are applied machine learning, deep neural network, time series and everything around NLP in the field of ecommerce and consumer internet. My research focus is on information retrieval involving neuroscience and deep reinforcement learning.\\nI like to listen to a lot of learning courses and read research papers involving deep learning. In my spare time I like to keep up with the news, read blogs on medium and watch a few sci-fi films.',\n",
       " 'Snehil started his entrepreneurial journey 14 years ago with the launch of a social networking site along with music and video streaming portals back in 2006, while he was still in school. In 2011 while pursuing engineering in Computer Science, he joined Letsbuy, an e-commerce startup, where he developed and launched their mobile app and site while mobile-commerce was still in its nascent stage in India. Letsbuy was later acquired by Flipkart in 2012.Snehil also co-founded Findyahan, a services marketplace, which was eventually acquired in 2016 by Zimmber. Snehil joined Zimmber as Vice President of Product & Marketing. Zimmber was later acquired by Quikr.',\n",
       " \"I have over 7 years of combined experience in the fields of data science and machine learning. I've led many data science projects in a wide array of industries. I mainly program in Python using its popular data science libraries.For deep learning, my go to framework is PyTorch. I’ve also worked a significant amount with relational databases and cloud environments.\\nWorked on diverse array of projects where I used my machine learning expertise to build and advise external clients on how to move forward with machine learning projects. I also advised on how to best collect and structure data.\\nOther than work, I write a significant amount with regards to AI. I’ve published several deep learning tutorials,focusing on the PyTorch framework. My articles are published on Medium under the publication A Coder’s Guide to AI.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"\"\"I have 3.5+ years of work experience and was working as a data scientist with 3 different organizations. I was responsible for using predictive modelling, data processing, and data mining algorithms to solve challenging business problems.\n",
    "My technology stack includes but not limited to, are python, machine learning, deep learning, time-series, web scraping, flask, FastAPI, snowflake SQL servers, deploying production based servers, keras, TensorFlow, hugging face, Big Data and Data Warehouses. In my career, my growth has been exponential, and I developed interpersonal skills, now I know how to handle a project end to end.\n",
    "My area of interests are applied machine learning, deep neural network, time series and everything around NLP in the field of ecommerce and consumer internet. My research focus is on information retrieval involving neuroscience and deep reinforcement learning.\n",
    "I like to listen to a lot of learning courses and read research papers involving deep learning. In my spare time I like to keep up with the news, read blogs on medium and watch a few sci-fi films.\"\"\",\n",
    "           \"Snehil started his entrepreneurial journey 14 years ago with the launch of a social networking site along with music and video streaming portals back in 2006, while he was still in school. In 2011 while pursuing engineering in Computer Science, he joined Letsbuy, an e-commerce startup, where he developed and launched their mobile app and site while mobile-commerce was still in its nascent stage in India. Letsbuy was later acquired by Flipkart in 2012.Snehil also co-founded Findyahan, a services marketplace, which was eventually acquired in 2016 by Zimmber. Snehil joined Zimmber as Vice President of Product & Marketing. Zimmber was later acquired by Quikr.\",\n",
    "           \"\"\"I have over 7 years of combined experience in the fields of data science and machine learning. I've led many data science projects in a wide array of industries. I mainly program in Python using its popular data science libraries.For deep learning, my go to framework is PyTorch. I’ve also worked a significant amount with relational databases and cloud environments.\n",
    "Worked on diverse array of projects where I used my machine learning expertise to build and advise external clients on how to move forward with machine learning projects. I also advised on how to best collect and structure data.\n",
    "Other than work, I write a significant amount with regards to AI. I’ve published several deep learning tutorials,focusing on the PyTorch framework. My articles are published on Medium under the publication A Coder’s Guide to AI.\"\"\"]\n",
    "           \n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8817027-fee8-4f1f-9e04-d68ad7ae3fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B.Tech / M.Tech degree in Computer Science from a premiere institute.\\nShould have 1 - 5 years of experience in designing, developing and deploying software, preferably Statistical and Machine Learning models.\\nAbility to work independently with strong problem solving skills.\\nShould have excellent knowledge in fundamentals of Machine Learning and Artificial Intelligence, especially in Regression, Forecasting and Optimization.\\nShould have excellent foundational knowledge in Probability, Statistics and Operations Research/Optimization techniques.\\nShould have hands on experience thorugh ML Lifecycle from EDA to model deployment.\\nShould have hands on experience data analysis tools like Jupyter, and packages like Numpy, Pandas, Matplotlib.\\nShould be hands-on in writing code that is reliable, maintainable, secure, performance optimized.\\nShould have good knowledge in Cloud Platforms and Service oriented architecture and design'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\"\"B.Tech / M.Tech degree in Computer Science from a premiere institute.\n",
    "Should have 1 - 5 years of experience in designing, developing and deploying software, preferably Statistical and Machine Learning models.\n",
    "Ability to work independently with strong problem solving skills.\n",
    "Should have excellent knowledge in fundamentals of Machine Learning and Artificial Intelligence, especially in Regression, Forecasting and Optimization.\n",
    "Should have excellent foundational knowledge in Probability, Statistics and Operations Research/Optimization techniques.\n",
    "Should have hands on experience thorugh ML Lifecycle from EDA to model deployment.\n",
    "Should have hands on experience data analysis tools like Jupyter, and packages like Numpy, Pandas, Matplotlib.\n",
    "Should be hands-on in writing code that is reliable, maintainable, secure, performance optimized.\n",
    "Should have good knowledge in Cloud Platforms and Service oriented architecture and design\"\"\"\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69bce2f1-2084-4d48-913f-3751eb777fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "def score_cos_sim(art1,art2):\n",
    "    scores = util.cos_sim(art1, art2)[0]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c491015-caa1-4d40-853a-354e5de2805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_inference(queries,document,model):\n",
    "    \n",
    "    score = dict()\n",
    "    \n",
    "    queries_encode = [model.encode(text) for text in queries]\n",
    "    document_encode = model.encode(document)\n",
    "    \n",
    "    for i,query in enumerate(queries_encode):\n",
    "        score[\"document_\"+str(i)] = score_cos_sim(query,document_encode)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7c589e-be36-43a0-8541-d56c42abb048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf0b5f0e4c848a893c026a3e6ea4045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0bab821c64592896f8a5ef9154220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf28f001b5a4cb285596dc0b538f416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8269e1c57da9424493225516e1675824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'document_0': tensor([0.8477]),\n",
       " 'document_1': tensor([0.7088]),\n",
       " 'document_2': tensor([0.8285])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_inference(queries,document,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc4ae2-523c-4b39-926d-cf26ee6328cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
